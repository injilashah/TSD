{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/injilashah/TSD/blob/master/dynaic_ts_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b6d5d2",
      "metadata": {
        "id": "84b6d5d2"
      },
      "outputs": [],
      "source": [
        "from dynamicts.data_loader import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbebdc63",
      "metadata": {
        "id": "fbebdc63"
      },
      "outputs": [],
      "source": [
        "\n",
        "loader = DataLoader(filepath=\"I:/CQAI/TSA/TSD/TSD/data/natural_gas_price.csv\", index_col=\"Date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb64c3d",
      "metadata": {
        "id": "bdb64c3d"
      },
      "outputs": [],
      "source": [
        "data =loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe078998",
      "metadata": {
        "id": "fe078998",
        "outputId": "786ba8ca-533a-467b-9b0d-216089c5ff6d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-07-06 23:24:56,415 - WARNING - Data is not regular. Index differences are not uniform.\n",
            "2025-07-06 23:24:56,418 - WARNING - Unique differences found: <TimedeltaArray>\n",
            "['1 days', '3 days', '4 days', '5 days', '2 days', '15 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loader.is_regular()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aad74a0",
      "metadata": {
        "id": "3aad74a0",
        "outputId": "edc063e5-ebed-4c18-d5ea-b160f7d5e164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing days: 2686\n",
            "DatetimeIndex(['1997-01-11', '1997-01-12', '1997-01-18', '1997-01-19',\n",
            "               '1997-01-25', '1997-01-26', '1997-02-01', '1997-02-02',\n",
            "               '1997-02-08', '1997-02-09',\n",
            "               ...\n",
            "               '2020-08-01', '2020-08-02', '2020-08-08', '2020-08-09',\n",
            "               '2020-08-15', '2020-08-16', '2020-08-22', '2020-08-23',\n",
            "               '2020-08-29', '2020-08-30'],\n",
            "              dtype='datetime64[ns]', length=2686, freq=None)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create complete date range from min to max\n",
        "full_range = pd.date_range(start=data.index.min(), end=data.index.max(), freq='D')\n",
        "\n",
        "# Find missing dates\n",
        "missing_dates = full_range.difference(data.index)\n",
        "\n",
        "print(\"Missing days:\", len(missing_dates))\n",
        "print(missing_dates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8330a583",
      "metadata": {
        "id": "8330a583"
      },
      "outputs": [],
      "source": [
        "diffs = data.index.to_series().diff().dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4915185d",
      "metadata": {
        "id": "4915185d",
        "outputId": "3219fa55-ac8e-41cb-ad69-5467543ab756"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "date\n",
              "1997-01-08   1 days\n",
              "1997-01-09   1 days\n",
              "1997-01-10   1 days\n",
              "1997-01-13   3 days\n",
              "1997-01-14   1 days\n",
              "              ...  \n",
              "2020-08-26   1 days\n",
              "2020-08-27   1 days\n",
              "2020-08-28   1 days\n",
              "2020-08-31   3 days\n",
              "2020-09-01   1 days\n",
              "Name: date, Length: 5952, dtype: timedelta64[ns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412aa8a6",
      "metadata": {
        "id": "412aa8a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "if diffs.nunique() == 1:\n",
        "    print(f\"Data is regular. Index differences are uniform: {diffs.iloc[0]}\")\n",
        "\n",
        "else:\n",
        "    print(\"Data is not regular. Index differences are not uniform.\")\n",
        "    print(f\"Unique differences found: {diffs.unique()}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cdddd98",
      "metadata": {
        "id": "8cdddd98",
        "outputId": "9d797459-742e-4189-8747-7ca150335a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Irregular gaps stored in dictionary\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "#Result dictionary\n",
        "grouped_dates = defaultdict(list)\n",
        "\n",
        "\n",
        "for date, delta in diffs.items():\n",
        "    #if delta != pd.Timedelta(days=1):  # Skip 1-day diffs\n",
        "    grouped_dates[str(delta)].append(str(date.date()))  # Convert Timestamp to string\n",
        "\n",
        "#Convert to  dictionary\n",
        "irregular_gaps = dict(grouped_dates)\n",
        "\n",
        "\n",
        "print(\"✅ Irregular gaps stored in dictionary\")\n",
        "print(irregular_gaps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9855b343",
      "metadata": {
        "id": "9855b343"
      },
      "source": [
        "Actual implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b698b5",
      "metadata": {
        "id": "f9b698b5"
      },
      "outputs": [],
      "source": [
        "print(\"Inferred frequency:\", data.index.freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0024abfe",
      "metadata": {
        "id": "0024abfe"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "def get_initial_unique_gaps(diffs):\n",
        "    grouped_dates = defaultdict(list)\n",
        "    for date, delta in diffs.items():\n",
        "        #if delta != base_delta:\n",
        "        grouped_dates[str(delta)].append(str(date.date()))\n",
        "    return dict(grouped_dates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3835f692",
      "metadata": {
        "id": "3835f692"
      },
      "outputs": [],
      "source": [
        "def compute_diffs_and_group(dates_list,iteration,key):\n",
        "    timestamps = pd.to_datetime(dates_list)\n",
        "    diffs = timestamps.to_series().diff().dropna()\n",
        "    print(f\"Iteration {iteration} Key {key} Unique differences:\", diffs.unique())\n",
        "    grouped = defaultdict(list)\n",
        "    for date, delta in diffs.items():\n",
        "        grouped[str(delta)].append(str(date.date()))\n",
        "    return grouped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367d2468",
      "metadata": {
        "id": "367d2468"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def iterative_pattern_discovery(irregular_gaps, data):\n",
        "    current_groups = irregular_gaps.copy()\n",
        "    final_discarded = defaultdict(list)\n",
        "    final_kept = defaultdict(list)\n",
        "    iteration = 1\n",
        "\n",
        "    while current_groups:\n",
        "        next_groups = {}\n",
        "\n",
        "        for delta_key, dates_list in current_groups.items():\n",
        "            grouped = compute_diffs_and_group(dates_list)\n",
        "\n",
        "            for new_delta, new_dates in grouped.items():\n",
        "                count = len(new_dates)\n",
        "                key = f\"{delta_key} → {new_delta}\"\n",
        "\n",
        "                if count > 1:\n",
        "                    next_groups[key] = new_dates\n",
        "                    final_kept[key].extend(new_dates)\n",
        "                else:\n",
        "                    final_discarded[key].extend(new_dates)\n",
        "\n",
        "        current_groups = next_groups\n",
        "        iteration += 1\n",
        "\n",
        "    # Count unique discarded dates\n",
        "    discarded_dates = set(date for dates in final_discarded.values() for date in dates)\n",
        "    total_discarded = len(discarded_dates)\n",
        "\n",
        "    print(f\"\\nTotal entries in dataset: {len(data)}\")\n",
        "    print(f\"Total discarded unique dates: {total_discarded}\")\n",
        "    print(f\"Percentage of discarded entries: {(total_discarded / len(data)) * 100:.2f}%\")\n",
        "\n",
        "    return dict(final_kept), dict(final_discarded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "569bf4de",
      "metadata": {
        "id": "569bf4de",
        "outputId": "c7d845b5-a068-42f8-f2d2-268e1f254884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Unique differences: <TimedeltaArray>\n",
            "['1 days', '3 days', '4 days', '5 days', '2 days', '15 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n"
          ]
        }
      ],
      "source": [
        "# Assume df is your DataFrame with a datetime index\n",
        "diffs = data.index.to_series().diff().dropna()\n",
        "\n",
        "\n",
        "#Get initial irregular gaps\n",
        "irregular_gaps = get_initial_unique_gaps(diffs)\n",
        "print(\"Initial Unique differences:\", diffs.unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460357cd",
      "metadata": {
        "id": "460357cd"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "def get_dates_by_unique_diffs(data):\n",
        "    diffs = data.index.to_series().diff().dropna()\n",
        "    delta_to_dates = defaultdict(list)\n",
        "\n",
        "    for i, delta in enumerate(diffs):\n",
        "        current_date = data.index[i]  # i (not i+1) because diff aligns with the current row\n",
        "        delta_to_dates[str(delta)].append(current_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "    # Save to JSON\n",
        "    with open(\"unique_deltas_with_dates.json\", \"w\") as f:\n",
        "        json.dump(delta_to_dates, f, indent=4)\n",
        "\n",
        "    return delta_to_dates\n",
        "get_dates_by_unique_diffs(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea06322b",
      "metadata": {
        "id": "ea06322b",
        "outputId": "01361001-6c86-442b-af16-784c345061be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1 Key 1 Unique differences: <TimedeltaArray>\n",
            "['1 days', '4 days', '5 days', '6 days', '3 days', '19 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 1 Key 2 Unique differences: <TimedeltaArray>\n",
            "['7 days', '14 days', '21 days', '42 days']\n",
            "Length: 4, dtype: timedelta64[ns]\n",
            "Iteration 1 Key 3 Unique differences: <TimedeltaArray>\n",
            "[ '41 days',  '57 days', '140 days',  '28 days',  '55 days',  '43 days',\n",
            "  '64 days', '111 days',   '7 days',  '15 days',  '48 days',  '35 days',\n",
            "  '63 days',  '22 days',  '20 days',  '42 days',  '36 days',  '98 days',\n",
            " '112 days',  '14 days',  '62 days',  '50 days',  '13 days',  '34 days',\n",
            " '945 days',  '70 days']\n",
            "Length: 26, dtype: timedelta64[ns]\n",
            "Iteration 1 Key 4 Unique differences: <TimedeltaArray>\n",
            "[ '364 days',   '36 days',  '328 days',   '30 days',  '194 days',  '147 days',\n",
            "   '28 days',    '7 days',  '329 days',  '219 days',  '145 days', '4382 days',\n",
            "  '187 days']\n",
            "Length: 13, dtype: timedelta64[ns]\n",
            "Iteration 1 Key 5 Unique differences: <TimedeltaArray>\n",
            "[   '7 days',  '915 days',  '365 days',  '181 days',  '358 days', '1645 days',\n",
            "  '141 days',   '33 days',  '331 days',   '28 days',  '329 days',  '364 days',\n",
            "  '223 days',  '184 days',  '147 days',   '27 days',  '330 days', '1280 days',\n",
            "  '512 days',   '59 days']\n",
            "Length: 20, dtype: timedelta64[ns]\n",
            "Iteration 1 Key 6 Unique differences: <TimedeltaArray>\n",
            "[]\n",
            "Length: 0, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 1 Unique differences: <TimedeltaArray>\n",
            "[ '1 days',  '5 days',  '6 days',  '7 days', '12 days', '19 days', '20 days',\n",
            "  '4 days']\n",
            "Length: 8, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 2 Unique differences: <TimedeltaArray>\n",
            "['7 days', '14 days', '21 days', '28 days', '42 days']\n",
            "Length: 5, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 3 Unique differences: <TimedeltaArray>\n",
            "[ '41 days',  '57 days', '140 days',  '28 days',  '55 days',  '43 days',\n",
            "  '64 days', '111 days',   '7 days',  '15 days',  '48 days',  '35 days',\n",
            "  '63 days',  '22 days',  '20 days',  '42 days',  '36 days',  '98 days',\n",
            " '112 days',  '14 days',  '62 days',  '50 days',  '13 days',  '34 days',\n",
            " '945 days',  '70 days']\n",
            "Length: 26, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 4 Unique differences: <TimedeltaArray>\n",
            "[  '28 days',    '7 days',  '329 days',  '364 days',   '36 days',  '183 days',\n",
            "  '145 days',   '30 days',  '187 days',  '147 days',  '219 days',  '334 days',\n",
            " '1421 days',  '217 days']\n",
            "Length: 14, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 5 Unique differences: <TimedeltaArray>\n",
            "['539 days', '7 days', '1645 days', '1827 days', '59 days']\n",
            "Length: 5, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 6 Unique differences: <TimedeltaArray>\n",
            "['7 days', '21 days', '42 days', '35 days', '28 days', '77 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 7 Unique differences: <TimedeltaArray>\n",
            "[ '42 days',  '56 days',  '91 days',  '49 days',  '28 days',  '63 days',\n",
            "  '84 days',  '35 days',  '21 days',  '98 days',  '14 days', '140 days',\n",
            " '133 days', '112 days', '966 days',  '70 days']\n",
            "Length: 16, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 8 Unique differences: <TimedeltaArray>\n",
            "['364 days', '1099 days', '273 days', '91 days', '728 days', '2191 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 9 Unique differences: <TimedeltaArray>\n",
            "['98 days', '364 days', '1365 days', '462 days', '1827 days']\n",
            "Length: 5, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 10 Unique differences: <TimedeltaArray>\n",
            "['98 days', '637 days', '1092 days', '462 days', '1827 days', '1365 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 11 Unique differences: <TimedeltaArray>\n",
            "['1463 days', '364 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 12 Unique differences: <TimedeltaArray>\n",
            "['364 days', '1099 days', '1463 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 13 Unique differences: <TimedeltaArray>\n",
            "['1099 days', '1092 days', '735 days', '4018 days']\n",
            "Length: 4, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 14 Unique differences: <TimedeltaArray>\n",
            "['1099 days', '1827 days', '1092 days', '4018 days']\n",
            "Length: 4, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 15 Unique differences: <TimedeltaArray>\n",
            "['2457 days', '1092 days', '469 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 16 Unique differences: <TimedeltaArray>\n",
            "['364 days', '1827 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 17 Unique differences: <TimedeltaArray>\n",
            "['729 days', '1462 days', '365 days', '364 days', '1098 days', '2191 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 18 Unique differences: <TimedeltaArray>\n",
            "['2191 days', '1827 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 19 Unique differences: <TimedeltaArray>\n",
            "['2926 days', '1092 days', '735 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 20 Unique differences: <TimedeltaArray>\n",
            "['231 days', '364 days', '1463 days', '133 days', '1232 days', '2191 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 21 Unique differences: <TimedeltaArray>\n",
            "['1827 days', '364 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 22 Unique differences: <TimedeltaArray>\n",
            "['1631 days', '2387 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 23 Unique differences: <TimedeltaArray>\n",
            "['1092 days', '2926 days', '1827 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 24 Unique differences: <TimedeltaArray>\n",
            "['364 days', '1463 days', '1827 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 25 Unique differences: <TimedeltaArray>\n",
            "['1827 days', '364 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 26 Unique differences: <TimedeltaArray>\n",
            "['1827 days', '364 days', '2191 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 27 Unique differences: <TimedeltaArray>\n",
            "['2926 days', '1092 days', '1827 days']\n",
            "Length: 3, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 28 Unique differences: <TimedeltaArray>\n",
            "['1092 days', '1827 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 29 Unique differences: <TimedeltaArray>\n",
            "['1092 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 30 Unique differences: <TimedeltaArray>\n",
            "['70 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 31 Unique differences: <TimedeltaArray>\n",
            "['364 days', '728 days', '735 days', '5334 days']\n",
            "Length: 4, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 32 Unique differences: <TimedeltaArray>\n",
            "['6209 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 33 Unique differences: <TimedeltaArray>\n",
            "['5476 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 34 Unique differences: <TimedeltaArray>\n",
            "['1826 days', '366 days', '1461 days', '365 days', '66 days', '7 days']\n",
            "Length: 6, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 35 Unique differences: <TimedeltaArray>\n",
            "['1827 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 36 Unique differences: <TimedeltaArray>\n",
            "['1827 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 37 Unique differences: <TimedeltaArray>\n",
            "['2191 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 38 Unique differences: <TimedeltaArray>\n",
            "['364 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Iteration 2 Key 39 Unique differences: <TimedeltaArray>\n",
            "['2191 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "\n",
            "Total entries in dataset: 5953\n",
            "Total discarded unique dates: 30\n",
            "Percentage of discarded entries: 0.50%\n"
          ]
        }
      ],
      "source": [
        "#Run iterative analysis\n",
        "patterns, no_patterns = iterative_pattern_discovery(irregular_gaps,data)\n",
        "\n",
        "\n",
        "import json\n",
        "with open(\"kept_patterns.json\", \"w\") as f1, open(\"discarded_patterns.json\", \"w\") as f2:\n",
        "    json.dump(patterns, f1,  indent=4)\n",
        "    json.dump(no_patterns, f2, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16062500",
      "metadata": {
        "id": "16062500"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def iterative_pattern_discovery(irregular_gaps, data):\n",
        "    current_groups = irregular_gaps.copy()\n",
        "    final_discarded = defaultdict(list)\n",
        "    final_kept = defaultdict(list)\n",
        "    classified_dates = set()  # Track already classified dates\n",
        "    iteration = 1\n",
        "\n",
        "    while current_groups:\n",
        "        next_groups = {}\n",
        "        which_key = 1\n",
        "\n",
        "        for delta_key, dates_list in current_groups.items():\n",
        "            grouped = compute_diffs_and_group(dates_list,iteration,which_key)\n",
        "\n",
        "            for new_delta, new_dates in grouped.items():\n",
        "                # Remove dates that have already been classified\n",
        "                new_dates = [d for d in new_dates if d not in classified_dates]\n",
        "                if not new_dates:\n",
        "                    continue\n",
        "\n",
        "                count = len(new_dates)\n",
        "                key = f\"{delta_key} → {new_delta}\"\n",
        "\n",
        "                if count > 1:\n",
        "                    next_groups[key] = new_dates\n",
        "                    final_kept[key].extend(new_dates)\n",
        "                else:\n",
        "                    final_discarded[key].extend(new_dates)\n",
        "\n",
        "                # Mark these dates as classified (either kept or discarded)\n",
        "                classified_dates.update(new_dates)\n",
        "            which_key +=1\n",
        "        current_groups = next_groups\n",
        "        iteration += 1\n",
        "\n",
        "    # Count unique discarded dates\n",
        "    discarded_dates = set(date for dates in final_discarded.values() for date in dates)\n",
        "    total_discarded = len(discarded_dates)\n",
        "\n",
        "    print(f\"\\nTotal entries in dataset: {len(data)}\")\n",
        "    print(f\"Total discarded unique dates: {total_discarded}\")\n",
        "    print(f\"Percentage of discarded entries: {(total_discarded / len(data)) * 100:.2f}%\")\n",
        "\n",
        "    return dict(final_kept), dict(final_discarded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a61c736",
      "metadata": {
        "id": "6a61c736",
        "outputId": "84ef637d-f4eb-4606-916d-f967875dee57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatetimeIndex(['1997-01-07', '1997-01-08', '1997-01-09', '1997-01-10',\n",
              "               '1997-01-13', '1997-01-14', '1997-01-15', '1997-01-16',\n",
              "               '1997-01-17', '1997-01-20',\n",
              "               ...\n",
              "               '2020-08-19', '2020-08-20', '2020-08-21', '2020-08-24',\n",
              "               '2020-08-25', '2020-08-26', '2020-08-27', '2020-08-28',\n",
              "               '2020-08-31', '2020-09-01'],\n",
              "              dtype='datetime64[ns]', name='date', length=5953, freq=None)"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed865f80",
      "metadata": {
        "id": "ed865f80",
        "outputId": "7417cfdd-e69e-40fb-e964-a0fd5052e5dd"
      },
      "outputs": [
        {
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRecursionError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# Example Usage:\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     93\u001b[39m     \u001b[38;5;66;03m# Create sample data with irregular gaps\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     result = \u001b[43manalyze_date_regularity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal Dates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mtotal_dates\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKept Dates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mkept_count\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36manalyze_date_regularity\u001b[39m\u001b[34m(dates)\u001b[39m\n\u001b[32m     64\u001b[39m         process_node(\n\u001b[32m     65\u001b[39m             \u001b[38;5;28msorted\u001b[39m(child_indices),\n\u001b[32m     66\u001b[39m             current_node,\n\u001b[32m     67\u001b[39m             level + \u001b[32m1\u001b[39m,\n\u001b[32m     68\u001b[39m             gap\n\u001b[32m     69\u001b[39m         )\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Start processing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Prepare results\u001b[39;00m\n\u001b[32m     75\u001b[39m kept_dates = dates[keep]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36manalyze_date_regularity.<locals>.process_node\u001b[39m\u001b[34m(date_indices, parent_node, level, gap_value)\u001b[39m\n\u001b[32m     62\u001b[39m     child_indices.add(date_indices[pos])\n\u001b[32m     63\u001b[39m     child_indices.add(date_indices[pos + \u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchild_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgap\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36manalyze_date_regularity.<locals>.process_node\u001b[39m\u001b[34m(date_indices, parent_node, level, gap_value)\u001b[39m\n\u001b[32m     62\u001b[39m     child_indices.add(date_indices[pos])\n\u001b[32m     63\u001b[39m     child_indices.add(date_indices[pos + \u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchild_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgap\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "    \u001b[31m[... skipping similar frames: analyze_date_regularity.<locals>.process_node at line 64 (2961 times)]\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36manalyze_date_regularity.<locals>.process_node\u001b[39m\u001b[34m(date_indices, parent_node, level, gap_value)\u001b[39m\n\u001b[32m     62\u001b[39m     child_indices.add(date_indices[pos])\n\u001b[32m     63\u001b[39m     child_indices.add(date_indices[pos + \u001b[32m1\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchild_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgap\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36manalyze_date_regularity.<locals>.process_node\u001b[39m\u001b[34m(date_indices, parent_node, level, gap_value)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Vectorized difference calculation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m diffs = np.diff(\u001b[43mdates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate_indices\u001b[49m\u001b[43m]\u001b[49m) / np.timedelta64(\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mD\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m unique_diffs, counts = np.unique(diffs, return_counts=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Create node\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5416\u001b[39m, in \u001b[36mIndex.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   5407\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   5408\u001b[39m             warnings.warn(\n\u001b[32m   5409\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUsing a boolean indexer with length 0 on an Index with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5410\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlength greater than 0 is deprecated and will raise in a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   5413\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   5414\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m5416\u001b[39m result = \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5417\u001b[39m \u001b[38;5;66;03m# Because we ruled out integer above, we always get an arraylike here\u001b[39;00m\n\u001b[32m   5418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.ndim > \u001b[32m1\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:381\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[33;03mThis getitem defers to the underlying array, which by-definition can\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[33;03monly handle list-likes, slices, and integer scalars\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# Use cast as we know we will get back a DatetimeLikeArray or DTScalar,\u001b[39;00m\n\u001b[32m    379\u001b[39m \u001b[38;5;66;03m# but skip evaluating the Union at runtime for performance\u001b[39;00m\n\u001b[32m    380\u001b[39m \u001b[38;5;66;03m# (see https://github.com/pandas-dev/pandas/pull/44624)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m result = cast(\u001b[33m\"\u001b[39m\u001b[33mUnion[Self, DTScalarOrNaT]\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib.is_scalar(result):\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:292\u001b[39m, in \u001b[36mNDArrayBackedExtensionArray.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type \"ExtensionArray\",\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# variable has type \"Union[int, slice, ndarray]\")\u001b[39;00m\n\u001b[32m    291\u001b[39m key = extract_array(key, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m key = \u001b[43mcheck_array_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m result = \u001b[38;5;28mself\u001b[39m._ndarray[key]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m lib.is_scalar(result):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:525\u001b[39m, in \u001b[36mcheck_array_indexer\u001b[39m\u001b[34m(array, indexer)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;66;03m# convert list-likes to array\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_like(indexer):\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     indexer = \u001b[43mpd_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    526\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexer) == \u001b[32m0\u001b[39m:\n\u001b[32m    527\u001b[39m         \u001b[38;5;66;03m# empty list is converted to float array by pd.array\u001b[39;00m\n\u001b[32m    528\u001b[39m         indexer = np.array([], dtype=np.intp)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\construction.py:351\u001b[39m, in \u001b[36marray\u001b[39m\u001b[34m(data, dtype, copy)\u001b[39m\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_sequence(data, dtype=dtype, copy=copy)\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inferred_dtype == \u001b[33m\"\u001b[39m\u001b[33minteger\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIntegerArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inferred_dtype == \u001b[33m\"\u001b[39m\u001b[33mempty\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FloatingArray._from_sequence(data, copy=copy)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:152\u001b[39m, in \u001b[36mBaseMaskedArray._from_sequence\u001b[39m\u001b[34m(cls, scalars, dtype, copy)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_from_sequence\u001b[39m(\u001b[38;5;28mcls\u001b[39m, scalars, *, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, copy: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Self:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     values, mask = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_coerce_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscalars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(values, mask)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\numeric.py:272\u001b[39m, in \u001b[36mNumericArray._coerce_to_array\u001b[39m\u001b[34m(cls, value, dtype, copy)\u001b[39m\n\u001b[32m    270\u001b[39m dtype_cls = \u001b[38;5;28mcls\u001b[39m._dtype_cls\n\u001b[32m    271\u001b[39m default_dtype = dtype_cls._default_np_dtype\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m values, mask, _, _ = \u001b[43m_coerce_to_data_and_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_cls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_dtype\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values, mask\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\arrays\\numeric.py:167\u001b[39m, in \u001b[36m_coerce_to_data_and_mask\u001b[39m\u001b[34m(values, dtype, copy, dtype_cls, default_dtype)\u001b[39m\n\u001b[32m    165\u001b[39m     values = np.array(values, copy=copy)\n\u001b[32m    166\u001b[39m inferred_type = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m values.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mis_string_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    168\u001b[39m     inferred_type = lib.infer_dtype(values, skipna=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inferred_type == \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:580\u001b[39m, in \u001b[36mis_string_dtype\u001b[39m\u001b[34m(arr_or_dtype)\u001b[39m\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    578\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_is_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr_or_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:1396\u001b[39m, in \u001b[36m_is_dtype\u001b[39m\u001b[34m(arr_or_dtype, condition)\u001b[39m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcondition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mi:\\CQAI\\TSA\\.venv\\Lib\\site-packages\\pandas\\core\\dtypes\\common.py:573\u001b[39m, in \u001b[36mis_string_dtype.<locals>.condition\u001b[39m\u001b[34m(dtype)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcondition\u001b[39m(dtype) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_string_or_object_np_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    574\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    575\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[31mRecursionError\u001b[39m: maximum recursion depth exceeded"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_date_regularity(dates):\n",
        "    \"\"\"Optimized version of date regularity analysis.\"\"\"\n",
        "    # Preprocessing with vectorized operations\n",
        "    dates = pd.DatetimeIndex(dates).dropna().unique().sort_values()\n",
        "    if len(dates) < 2:\n",
        "        return {\n",
        "            'kept_dates': dates,\n",
        "            'discarded_dates': pd.DatetimeIndex([]),\n",
        "            'is_regular': True,\n",
        "            'hierarchy_tree': {},\n",
        "            'iterations': []\n",
        "        }\n",
        "\n",
        "    # Initialize data structures\n",
        "    keep = np.ones(len(dates), dtype=bool)\n",
        "    date_to_index = {date: i for i, date in enumerate(dates)}\n",
        "    hierarchy_tree = defaultdict(list)\n",
        "    iterations = []\n",
        "    node_counter = 0\n",
        "\n",
        "    def process_node(date_indices, parent_node=None, level=0, gap_value=None):\n",
        "        nonlocal node_counter, keep\n",
        "\n",
        "        if len(date_indices) < 2:\n",
        "            return\n",
        "\n",
        "        # Vectorized difference calculation\n",
        "        diffs = np.diff(dates[date_indices]) / np.timedelta64(1, 'D')\n",
        "        unique_diffs, counts = np.unique(diffs, return_counts=True)\n",
        "\n",
        "        # Create node\n",
        "        current_node = node_counter\n",
        "        node_counter += 1\n",
        "        if parent_node is not None:\n",
        "            hierarchy_tree[parent_node].append((current_node, gap_value))\n",
        "\n",
        "        # Store iteration info\n",
        "        iterations.append({\n",
        "            'level': level,\n",
        "            'node_id': current_node,\n",
        "            'gap_value': gap_value,\n",
        "            'dates_count': len(date_indices),\n",
        "            'unique_gaps': unique_diffs[counts == 1].tolist(),\n",
        "            'non_unique_gaps': unique_diffs[counts > 1].tolist()\n",
        "        })\n",
        "\n",
        "        # Process unique gaps (to discard)\n",
        "        for gap in unique_diffs[counts == 1]:\n",
        "            pos = np.where(diffs == gap)[0][0]\n",
        "            discard_idx = date_indices[pos + 1]\n",
        "            keep[discard_idx] = False\n",
        "\n",
        "        # Process non-unique gaps (to recurse)\n",
        "        for gap in unique_diffs[counts > 1]:\n",
        "            gap_positions = np.where(diffs == gap)[0]\n",
        "            child_indices = set()\n",
        "            for pos in gap_positions:\n",
        "                child_indices.add(date_indices[pos])\n",
        "                child_indices.add(date_indices[pos + 1])\n",
        "            process_node(\n",
        "                sorted(child_indices),\n",
        "                current_node,\n",
        "                level + 1,\n",
        "                gap\n",
        "            )\n",
        "\n",
        "    # Start processing\n",
        "    process_node(np.arange(len(dates)))\n",
        "\n",
        "    # Prepare results\n",
        "    kept_dates = dates[keep]\n",
        "    discarded_dates = dates[~keep]\n",
        "    perc_discarded = len(discarded_dates) / len(dates) * 100\n",
        "\n",
        "    return {\n",
        "        'kept_dates': kept_dates,\n",
        "        'discarded_dates': discarded_dates,\n",
        "        'total_dates': len(dates),\n",
        "        'kept_count': len(kept_dates),\n",
        "        'discarded_count': len(discarded_dates),\n",
        "        'discarded_percentage': perc_discarded,\n",
        "        'iterations': iterations,\n",
        "        'hierarchy_tree': dict(hierarchy_tree),\n",
        "        'is_regular': perc_discarded <= 50\n",
        "    }\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample data with irregular gaps\n",
        "\n",
        "\n",
        "    result = analyze_date_regularity(data.index)\n",
        "\n",
        "    print(f\"Total Dates: {result['total_dates']}\")\n",
        "    print(f\"Kept Dates: {result['kept_count']}\")\n",
        "    print(f\"Discarded Dates: {result['discarded_count']} ({result['discarded_percentage']:.2f}%)\")\n",
        "    print(f\"Data is Regular: {result['is_regular']}\")\n",
        "\n",
        "    # Print iteration hierarchy\n",
        "    print(\"\\nHierarchy Tree (Parent -> Children with Gap Value):\")\n",
        "    for parent, children in result['hierarchy_tree'].items():\n",
        "        child_ids = [f\"{child_id} (gap={gap_val})\" for child_id, gap_val in children]\n",
        "        print(f\"Node {parent} -> {', '.join(child_ids)}\")\n",
        "\n",
        "    # Print iterations summary\n",
        "    print(\"\\nIteration Details:\")\n",
        "    for i, iter_info in enumerate(result['iterations']):\n",
        "        print(f\"\\nIteration {i} (Level {iter_info['level']}):\")\n",
        "        print(f\"  Node {iter_info['node_id']} - Gap Value: {iter_info['gap_value']}\")\n",
        "        print(f\"  Dates in Node: {iter_info['dates_count']}\")\n",
        "        print(f\"  Unique Gaps: {iter_info['unique_gaps']} (Discarded {len(iter_info['discarded_in_node'])} dates)\")\n",
        "        print(f\"  Non-Unique Gaps: {iter_info['non_unique_gaps']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcf5ede",
      "metadata": {
        "id": "4bcf5ede"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque  # For efficient stack operations\n",
        "\n",
        "def analyze_date_regularities(date_series, min_keep_ratio=0.5):\n",
        "    \"\"\"Iterative version to avoid recursion limits.\"\"\"\n",
        "    # Input validation\n",
        "    if not isinstance(date_series.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"Input must have a DatetimeIndex.\")\n",
        "\n",
        "    dates = date_series.index.sort_values().unique()\n",
        "    if len(dates) < 2:\n",
        "        return {\n",
        "            \"is_regular\": True,\n",
        "            \"total_dates\": len(dates),\n",
        "            \"num_kept\": len(dates),\n",
        "            \"num_discarded\": 0,\n",
        "            \"discard_ratio\": 0.0,\n",
        "            \"hierarchy\": {}\n",
        "        }\n",
        "\n",
        "    dates_np = dates.to_numpy()\n",
        "    keep_mask = np.ones(len(dates_np), dtype=bool)\n",
        "    hierarchy = {}\n",
        "    stack = deque()\n",
        "\n",
        "    # Initialize stack with root node\n",
        "    stack.append((np.arange(len(dates_np)), [], \"root\"))\n",
        "\n",
        "    while stack:\n",
        "        date_indices, level_path, parent_key = stack.pop()\n",
        "\n",
        "        if len(date_indices) < 2:\n",
        "            continue\n",
        "\n",
        "        # Calculate gaps\n",
        "        days_diff = (dates_np[date_indices[1:]] - dates_np[date_indices[:-1]]).astype('timedelta64[D]').astype(int)\n",
        "        unique_diffs, counts = np.unique(days_diff, return_counts=True)\n",
        "\n",
        "        for delta, count in zip(unique_diffs, counts):\n",
        "            path_key = level_path + [f\"{delta}_days\"]\n",
        "            full_key = \" -> \".join(path_key)\n",
        "\n",
        "            if count == 1:\n",
        "                # Discard end date of unique gap\n",
        "                gap_pos = np.where(days_diff == delta)[0][0]\n",
        "                discard_idx = date_indices[gap_pos + 1]\n",
        "                keep_mask[discard_idx] = False\n",
        "                hierarchy[full_key] = {\"count\": 1, \"delta\": f\"{delta}_days\", \"action\": \"discarded\"}\n",
        "            else:\n",
        "                # Schedule subgroup for processing\n",
        "                gap_indices = np.where(days_diff == delta)[0]\n",
        "                subgroup_indices = np.unique(np.concatenate([\n",
        "                    date_indices[gap_indices],\n",
        "                    date_indices[gap_indices + 1]\n",
        "                ]))\n",
        "                hierarchy[full_key] = {\"count\": len(subgroup_indices), \"delta\": f\"{delta}_days\"}\n",
        "                stack.append((subgroup_indices, path_key, full_key))\n",
        "\n",
        "    # Compile results\n",
        "    num_discarded = len(dates_np) - np.sum(keep_mask)\n",
        "    discard_ratio = num_discarded / len(dates_np)\n",
        "    is_regular = discard_ratio <= (1 - min_keep_ratio)\n",
        "\n",
        "    return {\n",
        "        \"is_regular\": is_regular,\n",
        "        \"total_dates\": len(dates_np),\n",
        "        \"num_kept\": np.sum(keep_mask),\n",
        "        \"num_discarded\": num_discarded,\n",
        "        \"discard_ratio\": discard_ratio,\n",
        "        \"min_keep_ratio\": min_keep_ratio,\n",
        "        \"hierarchy\": hierarchy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d255d16",
      "metadata": {
        "id": "8d255d16"
      },
      "outputs": [],
      "source": [
        "# Ensure the index is named 'date' for compatibility\n",
        "data.index.name = 'date'\n",
        "result = analyze_date_regularities(pd.Series(index=data.index))\n",
        "\n",
        "# Output\n",
        "print(json.dumps(result, indent=2))\n",
        "print(f\"Kept: {result['num_kept']}, Discarded: {result['num_discarded']}\")\n",
        "print(f\"Hierarchy keys: {list(result['hierarchy'].keys())[:5]}\")  # Show first 5 patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9760df7",
      "metadata": {
        "id": "e9760df7",
        "outputId": "b1ea1d04-5d24-4a2c-ad92-dbf644cdaf71"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1997-01-07</th>\n",
              "      <td>3.82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-01-08</th>\n",
              "      <td>3.80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-01-09</th>\n",
              "      <td>3.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-01-10</th>\n",
              "      <td>3.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997-01-13</th>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-26</th>\n",
              "      <td>2.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-27</th>\n",
              "      <td>2.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-28</th>\n",
              "      <td>2.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-08-31</th>\n",
              "      <td>2.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-09-01</th>\n",
              "      <td>2.22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5953 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            price\n",
              "date             \n",
              "1997-01-07   3.82\n",
              "1997-01-08   3.80\n",
              "1997-01-09   3.61\n",
              "1997-01-10   3.92\n",
              "1997-01-13   4.00\n",
              "...           ...\n",
              "2020-08-26   2.52\n",
              "2020-08-27   2.52\n",
              "2020-08-28   2.46\n",
              "2020-08-31   2.30\n",
              "2020-09-01   2.22\n",
              "\n",
              "[5953 rows x 1 columns]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b364938",
      "metadata": {
        "id": "0b364938",
        "outputId": "4ed4c28c-aa91-4833-dfe6-29deca74661c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatetimeIndex(['1997-01-07', '1997-01-08', '1997-01-09', '1997-01-10',\n",
              "               '1997-01-13', '1997-01-14', '1997-01-15', '1997-01-16',\n",
              "               '1997-01-17', '1997-01-20',\n",
              "               ...\n",
              "               '2020-08-19', '2020-08-20', '2020-08-21', '2020-08-24',\n",
              "               '2020-08-25', '2020-08-26', '2020-08-27', '2020-08-28',\n",
              "               '2020-08-31', '2020-09-01'],\n",
              "              dtype='datetime64[ns]', name='date', length=5953, freq=None)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fcc565f",
      "metadata": {
        "id": "6fcc565f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}